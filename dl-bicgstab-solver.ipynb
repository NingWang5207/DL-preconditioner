{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":38870.006481,"end_time":"2023-09-26T15:23:53.523935","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-09-26T04:36:03.517454","version":"2.4.0"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7527085,"sourceType":"datasetVersion","datasetId":4384157},{"sourceId":11846653,"sourceType":"datasetVersion","datasetId":7443280}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"43133878-ecae-468d-b0e9-cbd633d12cda","cell_type":"markdown","source":"##  Import Python modules","metadata":{}},{"id":"f03c5764","cell_type":"code","source":"import time\nimport torch\nimport imageio\nimport numpy as np\nfrom scipy import linalg\nimport scipy.io\nimport scipy.sparse as sp\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import splu,spilu\nfrom scipy.sparse import coo_matrix\nimport matplotlib.pyplot as plt\nfrom torch.nn import functional as F\nimport torch.nn as nn\nfrom scipy.sparse.linalg import spsolve\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":4.795834,"end_time":"2023-09-26T04:36:11.699284","exception":false,"start_time":"2023-09-26T04:36:06.903450","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"34778428-5816-4e21-b685-fb2de0a8de93","cell_type":"markdown","source":"## Impedance matrix and source term generating function","metadata":{}},{"id":"afbb3d36","cell_type":"code","source":"def matrix_ofd4(nx, nz, h, f, delta, c_vec):\n    \"\"\"\n    Construct the impedance matrix for 2D acoustic wave equation using a 4th-order \n    optimized finite difference (OFD) method with perfectly matched layer (PML) boundary conditions.\n\n    Parameters\n    ----------\n    nx : int\n        Number of grid nodes in the x-direction (including boundaries).\n    nz : int\n        Number of grid nodes in the z-direction (including boundaries).\n    h : float\n        Spatial grid spacing (uniform in both directions).\n    f : float\n        Frequency (Hz).\n    delta : int\n        Width (in grid points) of the PML absorbing layer.\n    c_vec : ndarray\n        Flattened 1D array of wave speeds at each grid point in the computational domain\n        (excluding boundaries), in row-major order.\n\n    Returns\n    -------\n    A : scipy.sparse.csr_matrix\n        The sparse complex impedance matrix of size (N, N), where \n        N = (nx - 2) * (nz - 2), is the problem scale.\n\n    Notes\n    -----\n    - This method assumes a rectangular 2D domain with a uniform grid.\n    - The PML is applied to all four boundaries.\n    - A 4th-order finite difference stencil is used for discretization.\n    - The resulting impedance matrix can be used in frequency-domain wave simulations.\n    \"\"\"\n\n    # imaginary unit\n    ii = 1j\n\n    # problem scale\n    N = (nx - 2) * (nz - 2)\n\n    # nonzero elements of impedance matrix\n    spn = 9 * (nx - 6) * (nz - 6) + 16 * (nx + nz - 12) + 14 * (nx + nz - 12) + 96\n\n    # sparse store: vector space\n    ai = np.zeros(spn, dtype=int)\n    aj = np.zeros(spn, dtype=int)\n    as_ = np.zeros(spn, dtype=complex)\n    pa = 0\n\n    # set angular frequency\n    omega = 2 * np.pi * f\n\n    # PML parameter: the ratio of reflection\n    R = 1e-3\n\n    for k in range(N):\n        # grid coordinate conversion: row rule\n        j = k // (nx - 2)\n        i = k - (nx - 2) * j\n\n        # set velocity: row rule\n        c = c_vec[k]\n\n        # set PML attenuation function\n        if i < delta - 1:\n            dx = (\n                    -3 * c / (2 * delta * h) * np.log(R) * ((delta - i - 1) / delta) ** 2\n            )\n            dxp = -3 * c / (delta * h) ** 2 * np.log(R) * (-(delta - i - 1) / delta)\n        elif i > nx - delta - 2:\n            dx = (\n                    -3 * c / (2 * delta * h) * np.log(R) * ((i - nx + delta + 2) / delta) ** 2\n            )\n            dxp = -3 * c / (delta * h) ** 2 * np.log(R) * ((i - nx + delta + 2) / delta)\n        else:\n            dx = 0\n            dxp = 0\n\n        if j < delta - 1:\n            dz = (\n                    -3 * c / (2 * delta * h) * np.log(R) * ((delta - j - 1) / delta) ** 2\n            )\n            dzp = -3 * c / (delta * h) ** 2 * np.log(R) * (-(delta - j - 1) / delta)\n        elif j > nz - delta - 2:\n            dz = (\n                    -3 * c / (2 * delta * h) * np.log(R) * ((j - nz + delta + 2) / delta) ** 2\n            )\n            dzp = -3 * c / (delta * h) ** 2 * np.log(R) * ((j - nz + delta + 2) / delta)\n        else:\n            dz = 0\n            dzp = 0\n\n        tx = 1 - ii * dx / omega\n        tz = 1 - ii * dz / omega\n\n        # matlab rule: index start from 1\n        kt = k + 1\n\n        # left1\n        if i != 0:\n            ai[pa] = kt\n            aj[pa] = kt - 1\n            as_[pa] = 4 / (3 * tx ** 2) - 2 * ii * dxp * h / (3 * omega * tx ** 3)\n            pa += 1\n\n        # left2\n        if i > 1:\n            ai[pa] = kt\n            aj[pa] = kt - 2\n            as_[pa] = -1 / (12 * tx ** 2) + ii * dxp * h / (12 * omega * tx ** 3)\n            pa += 1\n\n        # right1\n        if i != nx - 3:\n            ai[pa] = kt\n            aj[pa] = kt + 1\n            as_[pa] = 4 / (3 * tx ** 2) + 2 * ii * dxp * h / (3 * omega * tx ** 3)\n            pa += 1\n\n        # right2\n        if i < nx - 4:\n            ai[pa] = kt\n            aj[pa] = kt + 2\n            as_[pa] = -1 / (12 * tx ** 2) - ii * dxp * h / (12 * omega * tx ** 3)\n            pa += 1\n\n        # up1\n        if j != 0:\n            ai[pa] = kt\n            aj[pa] = kt - nx + 2\n            as_[pa] = 4 / (3 * tz ** 2) - 2 * ii * dzp * h / (3 * omega * tz ** 3)\n            pa += 1\n\n        # up2\n        if j > 1:\n            ai[pa] = kt\n            aj[pa] = kt - 2 * nx + 4\n            as_[pa] = -1 / (12 * tz ** 2) + ii * dzp * h / (12 * omega * tz ** 3)\n            pa += 1\n\n        # down1\n        if j != nz - 3:\n            ai[pa] = kt\n            aj[pa] = kt + nx - 2\n            as_[pa] = 4 / (3 * tz ** 2) + 2 * ii * dzp * h / (3 * omega * tz ** 3)\n            pa += 1\n\n        # down2\n        if j < nz - 4:\n            ai[pa] = kt\n            aj[pa] = kt + 2 * nx - 4\n            as_[pa] = -1 / (12 * tz ** 2) - ii * dzp * h / (12 * omega * tz ** 3)\n            pa += 1\n\n        # inner\n        ai[pa] = kt\n        aj[pa] = kt\n        as_[pa] = (omega * h / c) ** 2 - 5 / 2 * (1 / tx ** 2 + 1 / tz ** 2)\n        pa += 1\n\n    A = coo_matrix((as_, (ai - 1, aj - 1)), shape=(N, N)).tocsr()\n\n    return A\n\n\ndef source_ofd(f, f0, N, h, c_vec, s_loc):\n    \"\"\"\n    Generate the right-hand side (source term) for a frequency-domain 2D acoustic wave equation.\n\n    Parameters\n    ----------\n    f : float\n        Frequency (Hz) at which to evaluate the source.\n    f0 : float\n        Dominant frequency of the Ricker wavelet source.\n    N : int\n        Number of problem scale\n    h : float\n        Grid spacing (km).\n    c_vec : ndarray\n        1D array of wave speeds in the medium, in row-major order.\n    s_loc : int\n        Index of the source location in the flattened grid (0-based indexing).\n\n    Returns\n    -------\n    s : scipy.sparse.coo_matrix\n        A sparse column vector of shape (N, 1) representing the complex-valued\n        source term at frequency f.\n\n    Notes\n    -----\n    - The source is implemented using a frequency-domain Ricker wavelet.\n    - Only one non-zero element is present, located at the source index.\n    - The amplitude and phase are frequency-dependent.\n    \"\"\"\n\n    # initial unit\n    ii = 1j\n\n    # source location (count from 0)\n    s0 = s_loc\n\n    # source velocity\n    sc = c_vec[s0][0]\n\n    # Amplitude and phase\n    t0 = 0.12\n    Amp = 1e+5\n\n    s = np.sqrt(2) * Amp / (np.pi * f0) * (f / f0) ** 2 * np.exp(-(f / f0) ** 2) * \\\n        sp.coo_matrix(([-h ** 2 / sc ** 2 * np.exp(-ii * 2 * np.pi * f * t0)], ([s0], [0])), shape=(N, 1))\n\n    return s","metadata":{"papermill":{"duration":0.037501,"end_time":"2023-09-26T04:36:11.742920","exception":false,"start_time":"2023-09-26T04:36:11.705419","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d150fa50-b219-4bcf-b4f0-c5960c497adc","cell_type":"markdown","source":"# Basic parameter for generate impedance matrix and source","metadata":{}},{"id":"0fabedd7-6ee0-434f-aa11-7c9023905d6e","cell_type":"code","source":"# Number of grid points in the x-direction (including boundaries)\nnx = 306\n\n# Number of grid points in the z-direction (including boundaries)\nnz = 306\n\n # Number of problem scale (interior grid points)\nN = (nx - 2) * (nz - 2)\n\n# Spatial step size (uniform in both directions), in kilometers\nh = 0.025               \n\n# Source frequency (Hz)\nf = 40\n\n# Dominant frequency of the Ricker wavelet\nf0 = 20        \n\n# Number of PML layers on each boundary\ndelta = 10                  \n\n# ==================================================\n# Velocity model 1: homogeneous model (306 × 306)\n# ==================================================\nc_velocity = 4 * np.ones((nx - 2, nz - 2), dtype=float)\n# Flatten to a 1D array in row-major order\nc_vec = c_velocity.reshape(N, 1)\n\n# # =================================================\n# # # velocity model 2: two-layer model (258 × 258)\n# # =================================================\n# c_0, c_1 = 4 * np.ones(((nz - 2) // 2, nx - 2)), 5 * np.ones(((nz - 2) // 2, nx - 2))\n# c_velocity = np.concatenate((c_0, c_1), axis=0)\n# c_vec = c_velocity.reshape(N, 1)\n\n# # ================================================\n# # # velocity model 3: marmousi model (338 × 114)\n# # ================================================\n# marmousi = scipy.io.loadmat('/kaggle/input/wavefield-simulation-velocity-model/marmousi_vec.mat')\n# c_velocity = marmousi['c_mat']\n# c_vec = c_velocity.reshape(N, 1)\n\n# # ================================================\n# # # velocity model 4: bp2004 model (338 × 114)\n# # ================================================\n# bp2004 = scipy.io.loadmat('/kaggle/input/wavefield-simulation-velocity-model/bp2004_vec.mat')\n# c_velocity = bp2004['c_mat']\n# c_vec = c_velocity.reshape(N, 1)\n\n# Source location index (central position)\ns_loc = int((nx - 2) * (nz - 2) // 2 + (nx - 2) // 2)  \n\n# Generate the impedance matrix using 4th-order OFD\nA = matrix_ofd4(nx, nz, h, f, delta, c_vec)          \n\n# Generate the right-hand side source vector and convert to dense\nb = source_ofd(f, f0, N, h, c_vec, s_loc).todense()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"358b5e2c-21d6-4523-ac86-2257d1076af7","cell_type":"markdown","source":"## Trainging data (random version) generating function","metadata":{}},{"id":"9c0576db","cell_type":"code","source":"def generate_random_data(num_samples, impedance_matrix, nx, nz, seed=1234):\n    \"\"\"\n    Generate random complex-valued input-output training data for supervised learning.\n\n    Parameters:\n    -----------\n    num_samples : int\n        Number of training samples to generate.\n    impedance_matrix : scipy.sparse matrix or ndarray\n        The complex-valued system matrix A (e.g., impedance matrix) with shape (N, N),\n        where N is the spatial grid size in one direction (assumes square grid).\n\n    Returns:\n    --------\n    x_tensor : torch.Tensor\n        Input tensor of shape (num_samples, 2, N, N), where the 2 channels represent\n        the real and imaginary parts of a random complex input.\n    y_tensor : torch.Tensor\n        Output tensor of shape (num_samples, 2, N, N), representing the real and\n        imaginary parts of the corresponding matrix-vector product A @ x.\n    \"\"\"\n    x_data = np.zeros((num_samples, 2, nz-2, nx-2))    # Real and imaginary parts of input\n    y_data = np.zeros((num_samples, 2, nz-2, nx-2))    # Real and imaginary parts of output\n    np.random.seed(seed)\n    for i in range(num_samples):\n        # Generate random complex input field x with real and imaginary parts in [-1, 1]\n        x_real = 2 * np.random.rand(nz-2, nx-2) - 1\n        x_imag = 2 * np.random.rand(nz-2, nx-2) - 1\n        x_data[i, 0, :, :] = x_real\n        x_data[i, 1, :, :] = x_imag\n\n        x_complex = (x_real + 1j * x_imag).reshape(-1, 1)  # Flatten for matrix-vector multiplication\n\n        # Compute matrix-vector product: y = A @ x\n        y_complex = impedance_matrix @ x_complex\n\n        # Reshape result back to 2D grid and split into real/imaginary parts\n        y_data[i, 0, :, :] = y_complex.real.reshape(nz-2, nx-2)\n        y_data[i, 1, :, :] = y_complex.imag.reshape(nz-2, nx-2)\n\n        # Normalize both x and y by maximum absolute value of y (to prevent scale explosion)\n        max_val = np.max(np.abs(y_data[i]))\n        x_data[i] /= max_val\n        y_data[i] /= max_val\n\n    # Convert to PyTorch tensors\n    x_tensor = torch.tensor(x_data, dtype=torch.float32)\n    y_tensor = torch.tensor(y_data, dtype=torch.float32)\n\n    return x_tensor, y_tensor","metadata":{"papermill":{"duration":0.018061,"end_time":"2023-09-26T04:36:15.270214","exception":false,"start_time":"2023-09-26T04:36:15.252153","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"8c378220-152d-4d9c-988a-cde84e891e91","cell_type":"markdown","source":"## Complex media simulation requires integrating of generalized residuals","metadata":{}},{"id":"bbd5edda-3861-4e53-8b0f-297cc85c513b","cell_type":"code","source":"from scipy.ndimage import gaussian_filter\n\ndef generate_residual_data(num_samples, impedance_matrix, nx, nz, residuals, seed=1234):\n    \"\"\"\n    Generate complex-valued training data from residuals with small perturbations,\n    suitable for supervised learning.\n\n    Parameters:\n    -----------\n    num_samples : int\n        Number of training samples to generate.\n    matrix : scipy.sparse matrix or ndarray\n        The complex-valued system matrix A with shape (N, N).\n    residuals : ndarray\n        Array of precomputed residuals with shape (num_samples, N).\n    nx : int\n        Number of grid points in the x-direction (original grid).\n    nz : int\n        Number of grid points in the z-direction (original grid).\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns:\n    --------\n    x_tensor : torch.Tensor\n        Input tensor of shape (num_samples, 2, nz-2, nx-2), representing real and\n        imaginary parts of perturbed residual input.\n    y_tensor : torch.Tensor\n        Output tensor of shape (num_samples, 2, nz-2, nx-2), representing real and\n        imaginary parts of matrix-vector product A @ x.\n\n    Notes\n    -----\n    - Her the generalized residuals are obtained by running the ordinary bicgstab iterations.\n    - This is a initial demonstration, more details are to be developed further！\n    \"\"\"\n\n    np.random.seed(seed)\n    N = (nx - 2) * (nz - 2)\n    x_data = np.zeros((num_samples, 2, nz - 2, nx - 2))  # Real and imaginary input\n    y_data = np.zeros((num_samples, 2, nz - 2, nx - 2))  # Real and imaginary output\n\n    for i in range(num_samples):\n        # Generate small smooth complex perturbation\n        delta_real = gaussian_filter(2 * np.random.rand(N, 1) - 1, sigma=i/num_samples)\n        delta_imag = gaussian_filter(2 * np.random.rand(N, 1) - 1, sigma=i/num_samples)\n        X_delta = delta_real + 1j * delta_imag\n        # Normalize residual and add perturbation\n        residual_norm = residuals[i] / np.max(np.abs(residuals[i]))\n        X = residual_norm.reshape(-1, 1) + X_delta\n\n        # Compute output: Y = A @ X\n        Y = impedance_matrix @ X\n\n        # Reshape and separate real/imag parts\n        x_data[i,0,:,:] = X.real.reshape(x_data[i,0,:,:].shape)\n        x_data[i,1,:,:] = X.imag.reshape(x_data[i,1,:,:].shape)\n        y_data[i,0,:,:] = Y.real.reshape(y_data[i,0,:,:].shape)\n        y_data[i,1,:,:] = Y.imag.reshape(y_data[i,1,:,:].shape)\n\n        # Normalize input and output by max abs value in output\n        max_val = np.max(np.abs(y_data[i]))\n        x_data[i] /= max_val\n        y_data[i] /= max_val\n\n    # Convert to PyTorch tensors\n    x_tensor = torch.tensor(x_data, dtype=torch.float32)\n    y_tensor = torch.tensor(y_data, dtype=torch.float32)\n\n    return x_tensor, y_tensor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"addf6086-af08-457c-8e1c-0f299644e0a7","cell_type":"markdown","source":"## Generating Training data and Validation data","metadata":{}},{"id":"4caddbc4","cell_type":"code","source":"# Generate training and validation datasets from the impedance matrix\ntrainX, trainY = generate_random_data(num_samples=200, impedance_matrix=A, nx=nx, nz=nz)\nvalX, valY = generate_random_data(num_samples=50, impedance_matrix=A, nx=nx, nz=nz)\n\n# When the velocity model is complex, a mixed data set with residuals is required.\n# residual = [] # Record the generalized residual vector in BiCGTSAB (pk and qk).......\n# trainX_res, trainY_res = generate_residual_data(num_samples=len(residual), impedance_matrix=A, nx=nx, nz=nz, residuals=residual)\n# valX_res, valY_res = generate_residual_data(num_samples=len(residual), impedance_matrix=A, nx=nx, nz=nz, residuals=residual)\n# trainX, trainY = torch.cat([trainX, trainX_res], dim=0),  torch.cat([trainY, trainY_res], dim=0)\n# valX, valX = torch.cat([valX, valX_res], dim=0),  torch.cat([valY, valY_res], dim=0)\n\n# Print the shape of the generated datasets\nprint(\"Training input shape:\", trainX.shape)   # Expected: (200, 2, N, N)\nprint(\"Training label shape:\", trainY.shape)\nprint(\"Validation input shape:\", valX.shape)   # Expected: (10, 2, N, N)\nprint(\"Validation label shape:\", valY.shape)\n\n# Check numerical accuracy of matrix-vector multiplication for a sample\nsample_index = 96  # Index of the sample to validate\nx_complex = (trainX[sample_index, 0, :, :] + 1j * trainX[sample_index, 1, :, :]).numpy()\ny_complex = (trainY[sample_index, 0, :, :] + 1j * trainY[sample_index, 1, :, :]).numpy()\n\n# Flatten for multiplication: A @ x should give y\nx_flat = x_complex.reshape(-1, 1)\ny_flat = y_complex.reshape(-1, 1)\nerror = A @ x_flat - y_flat\n\n# Compute the norm of the error to verify consistency\nprint(\"Matrix-vector multiplication residual norm:\", np.linalg.norm(error))","metadata":{"papermill":{"duration":18.62797,"end_time":"2023-09-26T04:36:33.903278","exception":false,"start_time":"2023-09-26T04:36:15.275308","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"66319645-ce6d-44ef-8ea0-42250c699c85","cell_type":"markdown","source":"## Creating dataloader for batch training","metadata":{}},{"id":"73cdb64c","cell_type":"code","source":"# ================================\n# Prepare PyTorch DataLoaders\n# ================================\n\n# Construct training dataset: input is the solution (y), output is the source (x)\ntrain_dataset = TensorDataset(trainY, trainX)\n\n# Construct validation dataset: same format as training\nval_dataset = TensorDataset(valY, valX)\n\n# Define training DataLoader with batch shuffling enabled\ntrain_loader = DataLoader(\n    dataset=train_dataset,   # Dataset containing training samples\n    batch_size=16,           # Number of samples per batch\n    shuffle=True             # Shuffle the data at every epoch\n)\n\n# Define validation DataLoader with no shuffling\nval_loader = DataLoader(\n    dataset=val_dataset,     # Dataset containing validation samples\n    batch_size=16,           # Same batch size for consistency\n    shuffle=False            # No need to shuffle validation data\n)","metadata":{"papermill":{"duration":0.026366,"end_time":"2023-09-26T04:36:33.941133","exception":false,"start_time":"2023-09-26T04:36:33.914767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"06907e70-559f-4c0b-baea-d2ff82d27c12","cell_type":"markdown","source":"## Training data visualization","metadata":{}},{"id":"332f9a08","cell_type":"code","source":"# Select a sample index to visualize\ni = 5\n\n# Create subplots for input and output\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\n# Plot the real part of input vector x (channel 0)\nim1 = axs[0].imshow(trainY[i, 0, :, :].numpy(), cmap='bwr')\naxs[0].set_title('Real part of Input $Ax$', fontsize=12)\naxs[0].axis('off')\nfig.colorbar(im1, ax=axs[0], fraction=0.046, pad=0.04, shrink=0.3)\n\n# Plot the imaginary part of output vector Ax (channel 1)\nim2 = axs[1].imshow(trainX[i, 0, :, :].numpy(), cmap='bwr')\naxs[1].set_title('Real part of label $x$', fontsize=12)\naxs[1].axis('off')\nfig.colorbar(im2, ax=axs[1], fraction=0.046, pad=0.04, shrink=0.3)\nplt.show()","metadata":{"papermill":{"duration":0.766736,"end_time":"2023-09-26T04:36:34.718376","exception":false,"start_time":"2023-09-26T04:36:33.951640","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"673993fc-6a85-44ab-899b-d24759261619","cell_type":"markdown","source":"## Building UNet Architecture","metadata":{}},{"id":"36f447ee","cell_type":"code","source":"# ========================================\n# Basic Convolutional Block\n# Two 5x5 convolutions with Tanh activations\n# ========================================\nclass Conv(nn.Module):\n    def __init__(self, C_in, C_out):\n        super(Conv, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(C_in, C_out, kernel_size=5, stride=1, padding=2),\n            nn.Tanh(),\n            nn.Conv2d(C_out, C_out, kernel_size=5, stride=1, padding=2),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\n# ========================================\n# DownSampling Block\n# Halves spatial resolution using strided convolution\n# Channel size remains unchanged\n# ========================================\nclass DownSampling(nn.Module):\n    def __init__(self, C):\n        super(DownSampling, self).__init__()\n        self.Down = nn.Sequential(\n            nn.Conv2d(C, C, kernel_size=5, stride=2, padding=2),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.Down(x)\n\n# ========================================\n# UpSampling Block\n# Doubles spatial resolution via nearest interpolation\n# Reduces channels by half via 1x1 convolution\n# Concatenates with corresponding skip connection\n# ========================================\nclass UpSampling(nn.Module):\n    def __init__(self, C):\n        super(UpSampling, self).__init__()\n        self.Up = nn.Conv2d(C, C // 2, kernel_size=1, stride=1)\n\n    def forward(self, x, r):\n        up = F.interpolate(x, scale_factor=2, mode=\"nearest-exact\")\n        x = self.Up(up)\n        return torch.cat((x, r), dim=1)  # Concatenate along channel dimension\n\n# ========================================\n# Full U-Net Architecture\n# Symmetric encoder-decoder with skip connections\n# Input/Output channel: 2 (e.g. real + imaginary parts)\n# ========================================\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        # Encoder path (Downsampling)\n        self.C1 = Conv(2, 64)\n        self.D1 = DownSampling(64)\n        self.C2 = Conv(64, 128)\n        self.D2 = DownSampling(128)\n        self.C3 = Conv(128, 256)\n        self.D3 = DownSampling(256)\n        self.C4 = Conv(256, 512)\n        self.D4 = DownSampling(512)\n        self.C5 = Conv(512, 1024)\n\n        # Decoder path (Upsampling)\n        self.U1 = UpSampling(1024)\n        self.C6 = Conv(1024, 512)\n        self.U2 = UpSampling(512)\n        self.C7 = Conv(512, 256)\n        self.U3 = UpSampling(256)\n        self.C8 = Conv(256, 128)\n        self.U4 = UpSampling(128)\n        self.C9 = Conv(128, 64)\n\n        # Output layer\n        self.pred = nn.Conv2d(64, 2, kernel_size=5, stride=1, padding=2)\n        self.Th = nn.Tanh()\n\n    def forward(self, x):\n        # Encoder: extract hierarchical features\n        R1 = self.C1(x)\n        R2 = self.C2(self.D1(R1))\n        R3 = self.C3(self.D2(R2))\n        R4 = self.C4(self.D3(R3))\n        Y1 = self.C5(self.D4(R4))\n\n        # Decoder: reconstruct from features with skip connections\n        O1 = self.C6(self.U1(Y1, R4))\n        O2 = self.C7(self.U2(O1, R3))\n        O3 = self.C8(self.U3(O2, R2))\n        O4 = self.C9(self.U4(O3, R1))\n\n        # Final output with Tanh activation\n        return self.Th(self.pred(O4))","metadata":{"papermill":{"duration":0.033764,"end_time":"2023-09-26T04:36:34.762137","exception":false,"start_time":"2023-09-26T04:36:34.728373","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"c697edf6-e2c2-4e2b-ac75-9041ffb11ac9","cell_type":"markdown","source":"## Network training configuration","metadata":{}},{"id":"e20a8a77-b0e9-4f97-b44e-f84b19dc7e0e","cell_type":"code","source":"# Set computation device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate the U-Net model\nmodel = UNet().to(device)\n\n# Define Mean Squared Error loss function and Adam optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=4e-5)\n\n# Learning rate scheduler: halve the LR every 100 epochs\nscheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n\n# Number of training epochs\nnum_epochs = 300\n\n# Lists to store loss history\ntrain_loss = []\nvalid_loss = []\n\n# =========================\n# Begin training loop\n# =========================\nfor epoch in range(num_epochs):\n    model.train()\n    train_epoch_loss = 0.0\n\n    # Training phase\n    for batch_inputs, batch_targets in train_loader:\n        batch_inputs = batch_inputs.float().to(device)\n        batch_targets = batch_targets.float().to(device)\n\n        # Forward pass\n        outputs = model(batch_inputs)\n        loss = criterion(outputs, batch_targets)\n\n        # Backpropagation and optimization step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate training loss\n        train_epoch_loss += loss.item() * batch_inputs.size(0)\n\n    # Step the scheduler\n    scheduler.step()\n\n    model.eval()\n    valid_epoch_loss = 0.0\n\n    # Validation phase\n    with torch.no_grad():\n        for batch_inputs, batch_targets in val_loader:\n            batch_inputs = batch_inputs.float().to(device)\n            batch_targets = batch_targets.float().to(device)\n\n            # Forward pass\n            outputs = model(batch_inputs)\n            loss = criterion(outputs, batch_targets)\n\n            # Accumulate validation loss\n            valid_epoch_loss += loss.item() * batch_inputs.size(0)\n\n    # Normalize by total number of samples\n    train_epoch_loss /= len(train_loader.dataset)\n    valid_epoch_loss /= len(val_loader.dataset)\n\n    # Save loss values for plotting\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n\n    # Print epoch summary\n    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n          f\"Train Loss: {train_epoch_loss:.6f} | \"\n          f\"Valid Loss: {valid_epoch_loss:.6f}\")","metadata":{"papermill":{"duration":38734.461847,"end_time":"2023-09-26T15:22:09.233719","exception":false,"start_time":"2023-09-26T04:36:34.771872","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"abeb11b9-0369-42b2-bf2f-f8f40ac367e5","cell_type":"markdown","source":"## Loss function visualization","metadata":{}},{"id":"8f4cc38b-d9d2-4472-87a5-885233ac7861","cell_type":"code","source":"# Plot Training and Validation Loss\nplt.figure(figsize=(8, 6))\n\n# Use semilog-y scale to better visualize loss decay\nplt.semilogy(train_loss, 'b-', label='Training Loss', linewidth=2)\nplt.semilogy(valid_loss, 'r--',label='Validation Loss', linewidth=2)\n\n# Axis labels and title\nplt.xlabel('Epochs', fontsize=13)\nplt.ylabel('MSE loss', fontsize=13)\n\n# Add legend without frame\nplt.legend(frameon=False, fontsize=12)\n\n# Optional: Add grid for readability\nplt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":38734.461847,"end_time":"2023-09-26T15:22:09.233719","exception":false,"start_time":"2023-09-26T04:36:34.771872","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"237a10dc-ae47-4036-84df-91f486e880f7","cell_type":"markdown","source":"## Save the trained network weights","metadata":{}},{"id":"8584c209-68de-425c-950a-dd999e072ee6","cell_type":"code","source":"# Save Trained Model Weights\n\n# Define the path to save the model weights\nmodel_save_path = 'unet_weights.pth'\n\n# Save only the model parameters (recommended approach)\ntorch.save(model.state_dict(), model_save_path)\n\n# Confirm successful saving\nprint(f'Model weights saved successfully to \"{model_save_path}\"')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"371099bb-5605-4c2c-a7e0-713c47554719","cell_type":"markdown","source":"# Testing efficiency of DL preconditioners in local PyCharm","metadata":{}},{"id":"210d4b1d-4cdf-40c1-a615-a9c5da5a9a32","cell_type":"markdown","source":"## Taking homogeneous medium as an example","metadata":{}},{"id":"3a0c7cea-0a5e-4a28-9ca8-cdd8e279ba93","cell_type":"markdown","source":"## Step 1: Load model weights","metadata":{}},{"id":"3ec9a79b-defa-4450-829b-464616bfe63a","cell_type":"code","source":"# Instantiate the model\nmodel = UNet()\n\n# Define the path to the saved weights\nmodel_weights_path = 'unet_weights.pth'\n\n# Load model weights (ensure map to correct device)\nmodel.load_state_dict(torch.load(model_weights_path, map_location=device))\n\n# Move the model to the target device (CPU or GPU)\nmodel.to(device)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Confirmation message\nprint(f'Model weights loaded successfully from \\\"{model_weights_path}\\\"')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e0f7d4cc-20c5-4649-9220-25c3990b1bca","cell_type":"markdown","source":"## Step 2: Add network predictions to bicgstab method","metadata":{}},{"id":"153b7b6a-7b5c-45c5-ae14-d1834ba015f3","cell_type":"code","source":"class NetPreconditioner:\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n\n    def __call__(self, x):\n        \"\"\"\n        x: numpy array of shape (N,), where N = 304 * 304\n        Returns: numpy array of shape (N,)\n        \"\"\"\n        # Ensure x is complex\n        if np.iscomplexobj(x):\n            p = x\n        else:\n            p = x.astype(np.complex128)\n\n        # Convert to network input tensor\n        tensor = np.zeros((1, 2, 304, 304), dtype=np.float32)\n        tensor[0, 0] = p.real.reshape(304, 304)\n        tensor[0, 1] = p.imag.reshape(304, 304)\n        power = np.max(np.abs(tensor))\n        tensor = tensor / power\n        tensor = torch.from_numpy(tensor).float().to(self.device)\n\n        # Forward through network\n        with torch.no_grad():\n            output = self.model(tensor).cpu().numpy()\n        pred = output[0, 0] + 1j * output[0, 1]\n\n        return (pred * power).reshape(-1)\n\n\ndef get_net_linear_operator(model, device, shape):\n    preconditioner = NetPreconditioner(model, device)\n    return LinearOperator(\n        dtype=np.complex128,\n        shape=shape,\n        matvec=lambda x: preconditioner(x)\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9b48c7a5-0021-4f13-a7f5-585c8f2201bd","cell_type":"code","source":"def bicgstab(A, b, *, x0=None, tol=_NoValue, maxiter=None, M=None,\n             callback=None, atol=0., rtol=1e-5):\n    \"\"\"Use BIConjugate Gradient STABilized iteration to solve ``Ax = b``.\n\n    Parameters\n    ----------\n    A : {sparse matrix, ndarray, LinearOperator}\n        The real or complex N-by-N matrix of the linear system.\n        Alternatively, ``A`` can be a linear operator which can\n        produce ``Ax`` and ``A^T x`` using, e.g.,\n        ``scipy.sparse.linalg.LinearOperator``.\n    b : ndarray\n        Right hand side of the linear system. Has shape (N,) or (N,1).\n    x0 : ndarray\n        Starting guess for the solution.\n    rtol, atol : float, optional\n        Parameters for the convergence test. For convergence,\n        ``norm(b - A @ x) <= max(rtol*norm(b), atol)`` should be satisfied.\n        The default is ``atol=0.`` and ``rtol=1e-5``.\n    maxiter : integer\n        Maximum number of iterations.  Iteration will stop after maxiter\n        steps even if the specified tolerance has not been achieved.\n    M : {sparse matrix or Network, ndarray, LinearOperator}\n        Preconditioner for A.  The preconditioner should approximate the\n        inverse of A.  Effective preconditioning dramatically improves the\n        rate of convergence, which implies that fewer iterations are needed\n        to reach a given error tolerance.\n    callback : function\n        User-supplied function to call after each iteration.  It is called\n        as callback(xk), where xk is the current solution vector.\n    tol : float, optional, deprecated\n\n        .. deprecated:: 1.12.0\n           `bicgstab` keyword argument ``tol`` is deprecated in favor of\n           ``rtol`` and will be removed in SciPy 1.14.0.\n\n    Returns\n    -------\n    x : ndarray\n        The converged solution.\n    info : integer\n        Provides convergence information:\n            0  : successful exit\n            >0 : convergence to tolerance not achieved, number of iterations\n            <0 : parameter breakdown\n\n    rel_residuals : ndarray\n        Record the relative residual during the iterative process\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.sparse import csc_matrix\n    >>> from scipy.sparse.linalg import bicgstab\n    >>> R = np.array([[4, 2, 0, 1],\n    ...               [3, 0, 0, 2],\n    ...               [0, 1, 1, 1],\n    ...               [0, 2, 1, 0]])\n    >>> A = csc_matrix(R)\n    >>> b = np.array([-1, -0.5, -1, 2])\n    >>> x, exit_code = bicgstab(A, b, atol=1e-5)\n    >>> print(exit_code)  # 0 indicates successful convergence\n    0\n    >>> np.allclose(A.dot(x), b)\n    True\n\n    \"\"\"\n    A, M, x, b, postprocess = make_system(A, M, x0, b)\n    bnrm2 = np.linalg.norm(b)\n    rel_residuals = []\n    atol, _ = _get_atol_rtol('bicgstab', bnrm2, tol, atol, rtol)\n\n    if bnrm2 == 0:\n        return postprocess(b), 0, [0.0]\n\n    n = len(b)\n\n    dotprod = np.vdot if np.iscomplexobj(x) else np.dot\n\n    if maxiter is None:\n        maxiter = n*10\n\n    matvec = A.matvec\n    psolve = M.matvec\n\n    # These values make no sense but coming from original Fortran code\n    # sqrt might have been meant instead.\n    rhotol = np.finfo(x.dtype.char).eps**2\n    omegatol = rhotol\n\n    # Dummy values to initialize vars, silence linter warnings\n    rho_prev, omega, alpha, p, v = None, None, None, None, None\n\n    r = b - matvec(x) if x.any() else b.copy()\n    rtilde = r.copy()\n    rel_residuals.append(np.linalg.norm(r) / bnrm2)\n    for iteration in range(maxiter):\n        if np.linalg.norm(r) < atol:  # Are we done?\n            return postprocess(x), 0, rel_residuals\n\n        rho = dotprod(rtilde, r)\n        # if np.abs(rho) < rhotol:  # rho breakdown\n        #     return postprocess(x), -10, rel_residuals\n\n        if iteration > 0:\n            if np.abs(omega) < omegatol:  # omega breakdown\n                return postprocess(x), -11, rel_residuals\n\n            beta = (rho / rho_prev) * (alpha / omega)\n            p -= omega*v\n            p *= beta\n            p += r\n        else:  # First spin\n            s = np.empty_like(r)\n            p = r.copy()\n\n        phat = psolve(p)\n        v = matvec(phat)\n        rv = dotprod(rtilde, v)\n        if rv == 0:\n            return postprocess(x), -11, rel_residuals\n        alpha = rho / rv\n        r -= alpha*v\n        s[:] = r[:]\n\n        if np.linalg.norm(s) < atol:\n            x += alpha * phat\n            rel_residuals.append(np.linalg.norm(s) / bnrm2)\n            return postprocess(x), 0, rel_residuals\n\n        shat = psolve(s)\n        t = matvec(shat)\n        omega = dotprod(t, s) / dotprod(t, t)\n        x += alpha*phat\n        x += omega*shat\n        r -= omega*t\n        rho_prev = rho\n\n        rel_residuals.append(np.linalg.norm(r) / bnrm2)\n\n        if callback:\n            callback(x)\n\n    else:  # for loop exhausted\n        # Return incomplete progress\n        return postprocess(x), maxiter, rel_residuals","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e156132d-d9dc-4ee2-92ed-964ee83b993a","cell_type":"markdown","source":"## Step 3: Evaluate the computational time and number of iterations of UNet-BiCGSTAB method","metadata":{}},{"id":"352da9ee-2071-44ad-9d1c-ec0581340852","cell_type":"code","source":"# Convert trained UNet model to a linear operator to serve as preconditioner\nM = get_net_linear_operator(model, device, (N, N))  # UNet Preconditioner\n\n# Stopping criteria\nmax_iter = 5000  # Maximum number of BiCGSTAB iterations\ntol_res = 1e-5  # Tolerance of BiCGSTAB iterations\n# ------------------------------------------------------------\n# BiCGSTAB with no preconditioner\n# ------------------------------------------------------------\nstart_time = time.time()\nx_none, info_none, residuals_none = bicgstab(A, b, maxiter=max_iter, rtol=tol_res)\nelapsed_none = time.time() - start_time\n\nprint(\"BiCGSTAB without preconditioner\")\nprint(f\"Final residual: {residuals_none[-1]:.3e}\")\nprint(f\"Convergence info: {info_none}\")\nprint(f\"Iterative steps: {len(residuals_none)}\")\nprint(f\"Elapsed time: {elapsed_none:.2f} seconds\")\nprint(\"---------------------------------------------------------------\")\n\n# ------------------------------------------------------------\n# BiCGSTAB with UNet preconditioner\n# ------------------------------------------------------------\nstart_time = time.time()\nx_dl, info_dl, residuals_dl = bicgstab(A, b, M=M, maxiter=max_iter, rtol=tol_res)\nelapsed_dl = time.time() - start_time\n\nprint(\"BiCGSTAB with UNet preconditioner\")\nprint(f\"Final residual: {residuals_dl[-1]:.3e}\")\nprint(f\"Convergence info: {info_dl}\")\nprint(f\"Iterative steps: {len(residuals_dl)}\")\nprint(f\"Elapsed time: {elapsed_dl:.2f} seconds\")\n\n# ------------------------------------------------------------\n# Plotting convergence curves\n# ------------------------------------------------------------\nplt.figure(figsize=(8, 6))\nplt.semilogy(residuals_none, marker='o', label='No preconditioner')\nplt.semilogy(residuals_dl, marker='^', label='UNet preconditioner')\nplt.xlabel(\"iterative step\", fontsize=12)\nplt.ylabel(\"relative residual\", fontsize=12)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}